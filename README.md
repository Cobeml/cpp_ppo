# PPO Implementation in C++

A from-scratch implementation of Proximal Policy Optimization (PPO) in C++ without using any machine learning frameworks. This project includes a complete neural network library, PPO algorithm, and a scalable CartPole environment for testing.

## Project Structure

```
cpp_ppo/
â”œâ”€â”€ include/                    # Header files
â”‚   â”œâ”€â”€ neural_network/        # Neural network components
â”‚   â”‚   â”œâ”€â”€ matrix.hpp         # Matrix operations
â”‚   â”‚   â”œâ”€â”€ activation_functions.hpp
â”‚   â”‚   â”œâ”€â”€ dense_layer.hpp
â”‚   â”‚   â””â”€â”€ neural_network.hpp
â”‚   â”œâ”€â”€ ppo/                   # PPO algorithm components
â”‚   â”‚   â”œâ”€â”€ policy_network.hpp
â”‚   â”‚   â”œâ”€â”€ value_network.hpp
â”‚   â”‚   â”œâ”€â”€ ppo_buffer.hpp
â”‚   â”‚   â””â”€â”€ ppo_agent.hpp
â”‚   â”œâ”€â”€ environment/           # Test environment
â”‚   â”‚   â””â”€â”€ scalable_cartpole.hpp
â”‚   â””â”€â”€ utils/                 # Utility functions
â”‚       â”œâ”€â”€ learning_rate_scheduler.hpp
â”‚       â”œâ”€â”€ statistics.hpp
â”‚       â””â”€â”€ memory_pool.hpp
â”œâ”€â”€ src/                       # Source files (to be implemented)
â”‚   â”œâ”€â”€ neural_network/
â”‚   â”œâ”€â”€ ppo/
â”‚   â”œâ”€â”€ environment/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ examples/                  # Example usage
â”‚   â”œâ”€â”€ basic_training.cpp
â”‚   â”œâ”€â”€ test_neural_network.cpp
â”‚   â””â”€â”€ test_environment.cpp
â”œâ”€â”€ tests/                     # Unit tests
â”‚   â”œâ”€â”€ neural_network/
â”‚   â”œâ”€â”€ ppo/
â”‚   â”œâ”€â”€ environment/
â”‚   â””â”€â”€ integration/
â”œâ”€â”€ data/                      # Training data and logs
â”œâ”€â”€ build/                     # Build directory
â”œâ”€â”€ CMakeLists.txt            # Build configuration
â”œâ”€â”€ README.md                 # This file
â””â”€â”€ PPO_Implementation_Guide.md # Detailed implementation guide
```

## Features

### Neural Network Library
- **Matrix Operations**: Efficient matrix class with all necessary operations
- **Activation Functions**: ReLU, Tanh, Sigmoid, Linear, Softmax
- **Dense Layers**: Fully connected layers with backpropagation
- **Weight Initialization**: Xavier and He initialization methods
- **Gradient Clipping**: Prevents exploding gradients

### PPO Algorithm
- **Policy Network**: Neural network for action probability estimation
- **Value Network**: Neural network for state value estimation
- **Experience Buffer**: Stores and processes experience with GAE
- **Clipped Surrogate Objective**: Core PPO loss function
- **Entropy Bonus**: Encourages exploration
- **Multiple Epochs**: Mini-batch training with multiple passes

### Scalable Test Environment
- **CartPole Variant**: 5 difficulty levels from easy to very challenging
- **Configurable Physics**: Adjustable pole length, mass, gravity
- **Fast Testing**: Easy levels complete in ~150 steps
- **Deep Testing**: Hard levels can run 1000+ steps
- **Clear Metrics**: Episode length directly indicates performance

## Quick Start

### Prerequisites
- C++17 compatible compiler (GCC 7+, Clang 5+, MSVC 2017+)
- CMake 3.16+
- OpenMP (optional, for parallel processing)

### Building

```bash
# Clone the repository
git clone <repository-url>
cd cpp_ppo

# Create build directory
mkdir build && cd build

# Configure and build
cmake ..
make -j$(nproc)
```

### Running Examples

```bash
# Basic PPO training on CartPole
./basic_training

# Test neural network components
./test_neural_network

# Test environment scaling
./test_environment
```

### Running Tests

```bash
# Run all tests
make test

# Or run specific tests
./test_matrix
./test_ppo_buffer
./test_cartpole
```

## Implementation Status

### âœ… Completed
- [x] Project structure and build system
- [x] Header files for all components
- [x] Comprehensive implementation guide
- [x] Example training script

### ðŸš§ In Progress
- [ ] Matrix operations implementation
- [ ] Activation functions implementation
- [ ] Dense layer implementation
- [ ] Neural network implementation

### ðŸ“‹ TODO
- [ ] Policy network implementation
- [ ] Value network implementation
- [ ] PPO buffer implementation
- [ ] PPO agent implementation
- [ ] CartPole environment implementation
- [ ] Utility functions implementation
- [ ] Unit tests implementation
- [ ] Documentation and examples

## Development Phases

### Phase 1: Neural Network Foundation (Weeks 1-2)
1. Implement matrix operations
2. Implement activation functions
3. Implement dense layers
4. Implement basic neural network

### Phase 2: Test Environment (Week 2)
1. Implement CartPole physics
2. Add difficulty scaling
3. Add state normalization

### Phase 3: PPO Components (Weeks 3-4)
1. Implement policy network
2. Implement value network
3. Implement experience buffer

### Phase 4: PPO Algorithm (Weeks 4-5)
1. Implement core PPO agent
2. Add clipped surrogate loss
3. Add advantage estimation (GAE)

### Phase 5: Integration and Testing (Weeks 5-6)
1. Integrate all components
2. Add comprehensive tests
3. Performance optimization
4. Documentation

## Usage Example

```cpp
#include "ppo/ppo_agent.hpp"
#include "environment/scalable_cartpole.hpp"

int main() {
    // Create environment and agent
    ScalableCartPole env;
    env.set_difficulty_level(1);  // Easy level
    
    PPOAgent agent(4, 2);  // 4 state dims, 2 actions
    
    // Training loop
    for (int episode = 0; episode < 1000; ++episode) {
        auto state = env.reset();
        
        while (!env.is_done()) {
            int action = agent.select_action(Matrix(state));
            auto [next_state, reward] = env.step(action);
            
            agent.store_experience(Matrix(state), action, reward, 
                                 Matrix(next_state), env.is_done());
            
            if (agent.is_ready_for_update()) {
                agent.update();
            }
            
            state = next_state;
        }
    }
    
    return 0;
}
```

## Performance Expectations

### CartPole Difficulty Levels
- **Level 1 (Easy)**: Target 150+ average episode length
- **Level 2 (Standard)**: Target 200+ average episode length
- **Level 3 (Harder)**: Target 250+ average episode length
- **Level 4 (Much Harder)**: Target 400+ average episode length
- **Level 5 (Very Challenging)**: Target 600+ average episode length

### Learning Timeline
- **Episodes 0-1000**: Random performance, basic learning
- **Episodes 1000-5000**: Gradual improvement
- **Episodes 5000-10000**: Stable learning, solving easier levels
- **Episodes 10000+**: Consistent performance on harder levels

## Contributing

1. Follow the implementation phases outlined in the guide
2. Write comprehensive unit tests for each component
3. Ensure all tests pass before submitting changes
4. Document any new features or changes
5. Profile performance-critical sections

## Architecture Notes

### Design Principles
- **Modular Design**: Each component is self-contained and testable
- **Memory Efficiency**: Smart memory management with optional pooling
- **Numerical Stability**: Careful handling of floating-point operations
- **Scalability**: Easy to extend to different environments and algorithms

### Key Design Decisions
- **C++17 Standard**: Modern C++ features without bleeding-edge requirements
- **Header-Only Where Appropriate**: Templates and inline functions in headers
- **RAII**: Automatic resource management throughout
- **STL Usage**: Leverages standard library for containers and algorithms

## References

- [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)
- [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)
- [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)

## License

This project is licensed under the MIT License. See LICENSE file for details.

## Support

For questions about implementation or usage, please refer to the detailed guide in `PPO_Implementation_Guide.md` or open an issue in the repository. 